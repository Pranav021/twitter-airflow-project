# twitter-airflow-project

This project demonstrates the creation of a data pipeline using Twitter's API, Python, Apache Airflow, and AWS services. Below are the key steps and technologies involved.

## Key Steps:
### Data Extraction:

- Collected tweets using the Twitter API based on specific search criteria.
Data Transformation and Cleaning:

- Developed Python scripts to transform and clean the raw Twitter data for further analysis.

### Pipeline Automation:

Deployed Apache Airflow on an AWS EC2 instance to automate the data extraction and processing workflows.

### Data Storage:

- Stored the processed data in Amazon S3, utilizing its scalable and secure storage capabilities.

### Technologies Used:

- Twitter API: For extracting tweets.
- Python: For data transformation and cleaning.
- Apache Airflow: For orchestrating and automating the data pipeline.
- AWS EC2: For hosting the Airflow instance.
- Amazon S3: For storing the processed data.

This project provides hands-on experience in building and managing a data pipeline, leveraging powerful tools and cloud services to automate and streamline data processing tasks.
